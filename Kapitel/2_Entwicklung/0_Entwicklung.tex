\chapter{Entwicklung}
In diesem Kapitel werden die einzelnen Komponenten, die für eine komplette Ingestion notwendig sind entwickelt.
Nach \citeauthor{DL-Ing-Mgmt} ist der Ingestion-Prozess eines Data-Lake-Systems, als erster Schritt im Lebenszyklus der Daten, maßgebend dafür, wie gut die Daten später verwendet und verarbeitet werden können.
Um diese Aufgabe so gut wie möglich zu erfüllen, wurden im vorherigen Kapitel bereits die Anforderung festgelegt.
Daraus ergeben sich die drei Inhaltlichen Bereiche, nach denen die weiter Entwicklung aufgeteilt ist.

Der erste ist die Ingestion.
Diese beschreibt das erfassen von Informationen über eine Datenquelle und das Laden der Daten aus der Datenquelle.
Dabei werden die Daten noch nicht intern im Data Lake wieder abgelegt sondern existieren nur flüchtig im Arbeitsspeicher.

Die Deltaerkennung ist dafür zuständig die Unterschiede zwischen den geladenen Daten im Arbeitsspeicher und den aktuellen Daten aus dem internen Data Lake Speicher zu finden.
Aus diesen Unterschieden sollen die Änderungsdaten erstellt werden, die dann bei der weiteren Verarbeitung verwendet werden können.

Der dritte Bereich ist das Speichern von Daten in einem vordefinierten als auch in einem frei bestimmbaren Speichersystem zählt.
Der Unterschied dabei ist, dass im vordefinierten Speicher die Datenversionierung durch das Data-Lake-System unterstützt wird, während dies bei frei wählbaren nicht der Fall ist.

\section{Architektur}
\label{sec:arch}

Zu Anfang ist es sinnvoll, die Architektur des Systems festzulegen.
Diese bestimmt, welche Komponenten und Microservices entwickelt und verbunden werden müssen.
Der erste Schritt dabei ist es, die Aufagben aufzuteilen, die das System erfüllen soll.
Diese Aufgaben werden dann auf Microservices verteilt und in einer Architektur verbunden.
Die weitere Entwicklung des Systems stützt sich dann auf die fertige Architektur.

\subsection{Aufgabenverteilung}
Durch die Verwendung von \textit{Apache Spark} ist es nicht sinvoll, dass Laden der Daten, die Deltaerkennung und das Speichern zu trennen.
In \textit{Apache Spark} werden alle drei Arbeitsschritte auf einem \verb|Dataframe| ausgeführt.
Das heißt, dass dieses zwischen den Microservices ausgetauscht werden müsste, was zu einem erheblichen Aufwand führen würde.
Aus diesem Grund können die drei Bereiche aus der Einleitung nicht auch als Aufgabenverteilung verwendet werden.

Besser trennbare Aufgaben findet man bei der Betrachtung der technischen Seite.
Hierbei gibt es die Api, die Ingestion in \textit{Apache Spark} und das kontunierliche Ausführen.

Bei der \textbf{Api} handelt es sich um den Server für die Interaktion mit dem Data Lake System.
Durch die Anforderungen ist bereits festgelegt, dass dieser ein Web-Server mit einer REST-Schnittstelle ist.
Es geht zwar in dieser Arbeit nur um die Ingestion, aber diese Api kann für alle Funktionen des Datalakes verwendet werden.

Die \textbf{Ingestion} ist dafür zuständig, die Datenquellen zu verarbeiten und den kompletten Prozess von Laden bis Speichern der Daten in \textit{Apache Spark} auszuführen.
Die Ausführung soll für eine Datenquelle nur einmal gleichzeitig aber parallel für unterschiedliche laufen.

Bei einer zeitgesteuerten oder Datenstrom-Ingestion, muss die \textbf{kontinuierliche Ausführung} sichergestellt werden.
Für alle Datenquellen muss regelmäßig geprüft werden, ob für diese gerade eine Ingestion ausgeführt wird und ausgeführt werden sollten.
Falls keine ausgeführt wird aber sollte, wird die Ingestion für diese Datenquelle.

\subsection{Komponenten und Microservices}
Jede dieser Aufgaben kannn einem Microservice zugeordnet werden.
In \fref{fig:ingestion_arch} ist eine Architektur zu sehen, die Microservices für jede dieser aufgaben beinhaltet.
Der Api-Server übernimmt die REST-Schnittstelle, der Ingestion-Server kümmert sich um die Ausführung der Ingestion und der Continuation-Server stellt die kontinuierliche Ausführung sicher.

Neben diesen drei wird noch ein Nachrichtensystem benötigt, das dafür zuständig ist, die Kommunikation zwischen den  einzelnen Microservices zu koordinierten.
Dabei werden Nachrichten nicht direkt an einen anderen Mircoservices gesendet, sonder unter einem bestimmten Schlüssel an das Nachrichtensystem.
Das kümmert sich dann darum, dass alle Microservices, die Nachrichten mit diesem Schlüssel empfangen wollen, diese auch bekommen.
Ein Vorteil dabei ist, dass es sowhl einfach wird, einzelne Mircoservices horizontal zu skalieren, als auch neue in das System zu integrieren.

Zum Ablegen der internen Informationen wird eine Datenbank benötigt.
Alle Microservices haben zugriff auf diese Datenbank und können Daten in ihr beareiten.
Es handelt sich dabei aber nicht um den internen Speicher des Data Lakes sondern nur um Daten, die für den Betrieb des Systems benötigt werden.
Darunter fallen zum Beispiel Authentifizierungsdaten oder Verbindungsinformationen von Datenquellen.

\begin{figure}
  \centering
  \includegraphics{Grafiken/ingestion-arch.pdf}
  \caption{Architektur der Ingestion Komponenten}
  \label{fig:ingestion_arch}
\end{figure}

\section{Plugins}


\section{Datenquellen}
Ein weiterer Kernpunkt der Ingestion-Schnittstelle ist die Modellierung der Datenquellen.
Zur Entwicklung eines Datenmodells für die Datenquellen, wird als erstes betrachtet, welcher verschiedenen Typen von Datenquellen möglich sind.
Danach wird unter Zuhilfenahme des Ergebnisses ein Modell entwickelt.
Das fertige Datenmodell enthält dann neben den für den Betrieb benötigten Informationen auch Daten über den Verlauf der Ingestion einer Datenquelle.
Um die Behebung von Fehlern bei einer Ingestion einfacher zu gestalten werden alle Überarbeitungen einer Datenquelle in Revisionen festgehalten.

\subsection{Datenquellen-Typen}
\label{sec:datasource-types}

Da das System alle möglichen Datenquellen unterstützen soll, werden hier mögliche Typen dargestellt, mit denen sich alle Datenquellen abdecken lassen.
Dazu werden Merkmale betrachtet die Struktur und Art der Ingestion bestimmen.
Die Struktur der verwendeten Daten ist entweder strukturiert, semi- oder unstrukturiert.
Da in der Vorarbeit, dem Masterprojekt, \textit{Apache Spark} verwendet wurde, ist dieser Punkt bereits abgedeckt und fällt bei der Entwicklung nicht weiter ins Gewicht.

Die Art der Datenquelle beschreibt, wie Daten in das System gelangen.
Dazu gibt es zwei Merkmale, in denen sich die Ingestions unterscheiden können.
Das erste ist die Unterscheidung zwischen Push- und Pull-Prinzip.
Bei dem Push-Prinzip werden die zu speichernden Daten in der Anfrage an das System gesendet und bei dem Pull-Prinzip muss dass System die Daten aus einer Quelle laden.
Die zweite Unterscheidung findet statt in einmalige in kontinuierliche Ingestion.
Diese Unterscheidungen können, müssen aber nicht, von der Datenquelle abhängig sein.
Als Beispiel gibt es Datenströme, die laufend Daten senden und somit eine Ingestion benötigen, die auch laufend Daten annimmt.
Im Gegensatz dazu gibt es Datenbanken, bei denen das System die Daten aus der Quelle laden muss und somit die Ingestion sowohl einmalig als auch kontinuierlich sein kann.

Es ergeben sich vier Verarbeitungswege bei der Ingestion, die in \fref{fig:ingestion_types} zu sehen sind.
Sowohl für Push- und Pull-Prinzip kann eine einfache Ingestion ausgeführt werden, die sich für kontinuierliche Pull-Ingestions wiederholt.
Bei einer kontinuierlichen Ingestion, bei der Daten an das System gesendet werden, handelt es sich um Datenströme, die eine extra Verarbeitung erfordern.

\begin{figure}
  \centering
  \includegraphics{Grafiken/ingestion-types.pdf}
  \caption{Ingestion-Typen}
  \label{fig:ingestion_types}
\end{figure}

\subsection{Datenquellen-Model}

Bei der Entwicklung des Datenmodells können die Informationen zur Datenquelle in drei Teilmodelle unterteilt werden.
Die \textbf{Revision} enthält alle veränderlichen Informationen über die Datenquelle und kann nur erstellt aber nicht geändert werden.
Ein \textbf{Ingestion-Event} beschreibt die Ausführung einer Ingestion.
Und in der \textbf{Datasource} werden sowohl alle statischen Informationen als auch die Listen von Revisionen und Ingestion-Events gespeichert.

Im folgenden werden die Felder der Datenmodelle beschrieben.
Falls eine Information direkt aus \textit{Apache Spark} abgeleitet ist, wird auch Angegeben an welcher Stelle diese angewendet wird.

\paragraph{Revision}
\subparagraph{Nummer}
Eine aufsteigende Nummer, die die Revision identifiziert.
Sie startet bei 0 und jede Nummer ist für eine Datenquelle einzigartig.
Gleichzeitig spiegelt die Nummer auch den zeitlichen Verlauf wieder.
\subparagraph{Erstellungsdatum}
Das Datum an dem die Revision erstellt wurde.
Da eine Revision nicht geändert werden kann, ist das Erstellungsdatum einer Revision auch das Änderungsdatum der dazugehörigen Datenquelle.
\subparagraph{Name}
Ein Name, der der Datenquelle im Datalake gegebn wird.
Dieser dient nur dazu die Datenquelle als Benutzer besser identifizieren zu können.
\subparagraph{ID Spalte}
Der Name der Spalte oder des Feldes, das einen Datensatz eindeutig identifiziert.
Die ID Spalte wird für die zuordnung der Datensätze bei der Deltaerkennung benötigt.
\subparagraph{Spark Abhägigkeiten}
Eine Liste von Abhängigkeiten, die der Spark Session bei der Ingestion mitgegeben wird.
Diese Liste entspricht der Option "`spark.jars.packages"' die bei der Erstellung einer Spark Session gestzt wird.
\subparagraph{Quelldateien}
Bei der Ingestion können die Daten über das Push-Prinzip in Form von Dateien an den Server gesendet werden.
Die Liste der Quelldateien enthält die Pfade zu diesen Dateien.
\subparagraph{Typ beim Lesen}
Der Typ der zu lesenden Daten, wie in \ref{sec:datasource-types} beschrieben.
\subparagraph{Format beim Lesen}
Das Format in dem die Daten gelesen werden sollen.
Der Wert aus diesem Feld wird bei der Ingestion über die \verb|format| Methode eines Readers gesetzt.
\subparagraph{Optionen beim Lesen}
Eine Liste von Schlüssel-Wert-Paaren, die als Optionen des Readers in \textit{Apache Spark} gesetzt werden.
Über diese wird die verbindung zur Datenquelle definiert.
\subparagraph{Aktualisierung für Datenquelle}
Es kann für einen Datenquelle festgelegt werdne, dass diese Aktualisierungen für eine andere enthält.
Das ermöglicht die Nutzung von eigenen Change Data Capture Lösungen.
Dabei muss die Datenquelle aber dem internet Change Data Format entsprechen.
\subparagraph{Typ beim Schreiben}
Der Typ für das Schreiben der Daten legt fest, ob intern mit Versionierung oder in einen freien Speicher geschrieben werden soll.
\subparagraph{Format und Optionen beim Schreiben}
Das Format und die Optionen beim Schreiben funktionieren analog zu denen beim Lesen.
Sie werden jedoch nicht beim Reader gesetzt sondern beim Writer.
Gerade bei Datenströmen muss darauf geachtet werden, dass nicht in jedem Format ein Datenstrom geschrieben werden kann.
\subparagraph{Schreibmodus}
Der Modus in dem die Daten geschrieben werden.
Die Auswahlmöglichkeiten werden auch durch \textit{Apache Spark} festegelegt.
\subparagraph{Zeitsteuerungen}
Eine Liste die festlegt, zu welchen Zeitpunkte eine Ingestion der Datenquelle asugeführt werden soll.
\subparagraph{Plugin Abhängigkeiten}
Eine Liste von Abhängigkeiten, die von den Plugins der Datenquelle benötigt werden.
\subparagraph{Plugins}
Die Speicherorte der Plugins.

\paragraph{Ingestion-Event}
\subparagraph{Nummer}
Die Nummer um ein Ingestion-Event zu identifizieren.
Sie funktioniert genau wie die Nummer der Revisions.
\subparagraph{Status}
Der aktuelle Status in dem sich das Event befindet.
Er gibt auskunft, ob die Ingestion gestartet wurde, gerade läuft oder beendet wurde.
\subparagraph{Revisionsnummer}
Die Nummer der Revision, mit der die Inegstion gestartet wurde.
Mit Hilfe der Revisionsnummer ist es leichter Fehler in der Ingestion zu finden und zu beheben.
\subparagraph{Start- und Endzeit}
Die Zeitpunkte des Starts und Endes eines Ingestion-Durchlaufs.
Wenn die Ingestion noch in der Asuführung ist, ist keine Endzeit gesetzt.
\subparagraph{Fehler}
Die Fehlerausgabe, wenn die Ingestion nicht erfolgreich ausgeführt werden konnte.

\paragraph{Datasource}
\subparagraph{ID}
Eine Identifikationsnummer, die für jede Datenquelle eindeutig ist.
\subparagraph{Aktuelle Revision}
Die Nummer der aktuellen Revision.
Neue Ingestions werden mit den Informationen aus dieser Revision ausgeführt.
\subparagraph{Alle Revisionen}
Eine Liste aller Revisionen.
\subparagraph{Letzte Ingestion}
Die Nummer des zuletzt gestarteten Ingestion-Durchlaufs.
\subparagraph{Letzte erfolgreiche Ingestion}
Die Nummer des letzten erfolgreich abgeschlossenen Ingestion-Durchlauf.
\subparagraph{Alle Ingestion-Events}
Eine Liste aller Ingestion Events.

\input{Kapitel/2_Entwicklung/2_Ingestion}

\input{Kapitel/2_Entwicklung/3_Deltaerkennung}