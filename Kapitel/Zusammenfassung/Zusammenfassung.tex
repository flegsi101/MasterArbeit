\chapter{Zusammenfassung und Ausblick}

Die Aufgabenstellung dieser Arbeit war die Entwicklung einer Ingestion-Schnitt\-stelle für ein Data-Lake-System, dass im Kontext des HIT-Instituts an der Hochschule Niederrhein angewendet werden soll.
An diesem Institut werden Daten aus verschiedenen Quellen erfasst.
Es gibt klassische Datenbank-Systeme, Dateien oder REST-APIs.
Der Data Lake soll die zentrale Speicherlösung sein und Verarbeitungsprozesse unterstützen.
Dazu ist es neben der Unterstützung verschiedener Datenquellen auch nötig die Daten zu versioniern.
Dadurch verringert sich der Aufwand bei der weiteren Verarbeitung, da nur Änderungen betrachtet werden müssen.

In der vorliegenden Arbeit wurde eine generische Ingestion-Schnittstelle mit Versionierung für Data-Lake-Systeme entwickelt und mit funktionalen Tests und Benchmarks evaluiert.
Dafür wurde neben der Entwicklung der bestehende Prototyp des Data Lake auf eine Microservice-Archi\-tektur umgebaut.
Es wurden außerdem existierende Data-Lake-Systeme betrachtet und bewertet, ob diese als Lösung für die Problemstellung dienen könnten.
Daraus ergab sich, dass eine Eigenentwicklung notwendig ist.

Die Ingestion-Schnittstelle besteht aus drei Microservices.
Der Ingestion-Ser\-vice verwaltet das Laden von Daten in das System.
Dabei erlaubt die Verwendung von Spark und einem Plugin-System das Laden von Daten aus jeder Quelle.
Die kontinuierliche Ausführung von Ingestions wird durch den Continuation-Service sichergestellt.
Benutzer können über den API-Service mit der Ingestion-Schnittstelle interagieren.
Dieser besteht aus der Integration der neuen API in den Server des Prototyps.
Dadurch bleibt das System mit der Verwendung der alten API kompatibel.
Aus der Eingabe, die ein Benutzer an die API sendet, wird ein generelles Metadatenmodell aufgebaut.

Das Metadatenmodell zur Definition einer Datenquelle enthält alle Informationen über die Datenquelle, verwendete Plugins und zugehörige Ingestions.
Über dieses Modell werden die Informationen zu den Ingestions zwischen den Microservices ausgetauscht.

Neben der Ingestion-Schnittstelle besteht der Data Lake aus weiteren Komponenten.
Ein Kafka-Cluster wird für die Kommunikation im Data-Lake-System eingesetzt.
Diese kann auch für die Anbindung von Datenströmen verwendet werden.
Die Daten, die in den Data Lake geladen wurden, werden in einem HDFS gespeichert.
Daten mit Versionierung werden in einer Delta-Tabelle des Delta Lake abgelegt.
Für unversionierte Daten wird entweder das Parquet- oder, bei unstrukturierten Daten, das Ursprungsformat der Daten benutzt.
Für die Verwaltung von Metadaten kommt eine MongoDB-Datenbank zum Einsatz.
Die Datenverarbeitung geschieht über ein Spark-Cluster.

Die Evaluierung der Ingestion-Schnittstelle wurde in einem durch die gegebene Hardware möglichen Rahmen durchgeführt.
Hierfür stand kein leistungsstarkes Cluster zur Verfügung, um absolute Aussagen über die Leistung der Ingestion-Schnittstelle treffen zu können.
Daher ist lediglich ein relativer Vergleich ausgeführt worden, der Aussagen zur Skalierbarkeit des Ansatzes erlaubt.
Dabei wurden Benchmarks für verschiedene Datenmengen mit strukturierten und semistrukturierten Daten in zwei Szenarien ausgeführt.
Die Szenarien bilden dabei eine Situation, in der viele Änderungen an Daten geschehen und eine Situation, in der hauptsächlich Daten hinzugefügt werden ab.

Durch die Benchmarks und Tests wurde gezeigt, dass mit dieser Arbeit die Entwicklung der Ingestion-Schnittstelle noch nicht vollständig abgeschlossen ist.
Die implementierte Berechnung von Änderungsdaten ist in bestimmten Fällen sehr langsam.
Auch die Darstellung von Änderungen in verschachtelten Daten könnte durch ein anderes Vorgehen optimiert werden.
Somit ist die Versionierung in der Ingestion-Schnittstelle ein Thema, dass in einer anschließenden Arbeit genauer betrachtet werden kann.

Weiterhin sind die Metadaten, die während der Ingestion gesammelt werden, nicht ausreichend.
Um eine aussagekräftige Suche, Exploration und Analyse der Daten durchführen zu können, müssen direkt nach dem Laden weitere Metadaten automatisch extrahiert oder manuell durch einen Benutzer hinzugefügt werden.
Diese könnten zum Beispiel Informationen über den Inhalt oder die Struktur der Daten beinhalten.
Die in dieser Arbeit entwickelte Ingestion-Schnittstelle ist modular und erweiterbar aufgebaut, so dass in zukünftigen Arbeiten entsprechende Erweiterungen umgesetzt werden können.