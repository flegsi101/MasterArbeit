\section{Erfüllung der Anforderungen}

Die ersten Anforderungen die geprüft werden können, sind \namref{ANF_12} bis \nameref{ANF_15}.
Deren Erfüllung ist durch die entwickleten Komponenten gegeben.
Die Ingestion bietet eine REST-Schnittstelle, wie durch die Anforderung gefordert.
Es werden Metadaten während der Ingestion erfasst und die drei Microservices sind nicht voneinander abhängig und die Kommunikation mit Kafka ist so flexibel, dass es kein Problem gibt neue Services zu integrieren.
Auch \nameref{ANF_09}, das Speichern von Änderungen, ist durch die Verwendung von Delta Lake gegeben.

Die Erfüllung der restlichen Anforderungen muss durch Tests überprüft werden.
Hierzu werden Tests verwendet, die das komplette System im Zusammenhang überprüfen.
Hier bedeutet das, dass mehrere Ingestions ausgeführt werden, die alle Funktionen abdecken.
Mit den folgenden Schritten kann die korrekte Funktion einer Ingestion getestet werden: \begin{enumerate}
    \item Erstellung einer Datenquelle, zum Beispiel JSON-Dateien oder Postgres-Ta\-bellen, die in der Ingestion verwendet wird.
    \item Erstellung einer DatasourceDefinition für die Datenquelle.
    \item Vergleichen der DatasourceDefinition aus der Datenbank des Systems mit einer vordefinierten Soll-Definition.
    \item Starten der Ingestion.
    \item Vergleichen der Daten im System nach der Ingestion mit einem vordefinierten Soll-Datensatz
\end{enumerate}
Mit diesem Vorgehen kann auch die Aktualisierung von Daten getestet werden.
Dazu muss es zweimal hintereinander ausgeführt werden.
Im ersten Durchlauf werden die initialen Daten geladen und im zweiten wird dann entweder die gleiche Datenquelle mit veränderten Daten oder Änderungsdaten aus einer Update-Quelle geladen.

\subsection{Durchgeführte Tests}
\label{sec:tests-actual}
Nachfolgend werden die durchgeführten Tests geschildert.
Die ersten Tests sollen zeigen, dass sowohl das Laden von veränderten Daten als auch von Änderungsdaten für strukturierte und unstrukturierte Daten funktioniert.
Damit wird auch indirekt die korrekte Ingestion überprüft.

Ein verwendeter Datensatz muss aus einer Mindestanzahl an Einträgen bestehen, die alle möglichen Operation bei einer Änderung abdecken.
Der erste Eintrag bleibt in den Update-Daten unverändert, der zweite wird gelöscht und der dritte wird in einem Feld verändert.
Zusätzlich muss noch ein weiterer Eintrag hinzugefügt werden.
Damit Änderungen berechnet und eingepflegt werden können muss ein Feld festgelegt werden, dass als Id verwendet wird.
Hier muss auch der Fall geprüft werden, dass die Id sich in einem verschachtelten Feld befindet.

Für den Test von strukturierten Daten wird eine Postgres Datenbank und für semistrukturierte JSON-Dateien verwendet.
Damit werden der Upload von Dateien und das Laden von Daten aus einer Datenquelle abgedeckt.
Für beide wird sowohl die Ingestion von veränderten Daten der gleichen Quelle als auch Änderungsdaten ausgeführt.
Dabei werden jeweils eine Delta-Tabelle und Parquet als Speicherziel verwendet.
Das Speichern in einem externen Ziel wird hier nicht getestet, da die Versionierung nur für interne Speicher unterstützt wird.
Der erfolgreiche Abschluss der Tests hat gezeigt, dass diese Ingestion-Abläufe bei veränderten Daten funktionieren: \begin{itemize}
    \item Speichern des neuen Stands einer Datenquelle in Parquet,
    \item Einpflegen von Änderungsdaten aus einer externen Quelle in die Bestandsdaten,
    \item Berechnen von Änderungsdaten aus einem neuem Stand einer Datenquelle und Speichern dieser in einer Delta-Tabelle und
    \item Einpflegen von Änderungsdaten aus einer externen Quelle in eine Delta-Tabelle.
\end{itemize}

Bei den Tests für weitere Quellen genügt es nur noch eine Ingestion auszuführen.
Hier werden die Ingestion aus einem Kafka-Datenstrom und aus einer REST-Api überprüft.
Die Ingestion des Datenstroms verwendet dabei ein After-Load-Plugin, um die tatsächlichen Daten aus der Nachricht zu extrahieren.
Das Load-Plugin wird bei der Ingestion aus einer REST-API mit geprüft.
Auch diese Tests sind erfolgreich durchgelaufen.