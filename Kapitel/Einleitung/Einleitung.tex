\chapter{Einleitung}

Heutzutage spielen Daten und deren Verarbeitung in vielen Bereichen eine immer wichtigere Rolle.
Im \textit{Rethink Data Report 2020} \textcite{rethink_data_2020} wurde eine Studie durchgeführt, die eine Steigerung von 42\% der Menge an anfallenden Daten pro Jahr prognostiziert.
Dies wird unter anderem auf den vermehrten Einsatz von IoT-Geräten, immer ausführlichere Datenanalysen und die einfachere werdende Anwendung von Cloud-Speichern zurückgeführt.
Dabei besteht die Herausforderung, Daten in verschiedensten Formaten in großen Mengen zu verwalten und zu verwenden.

Als eine Lösung für das Problem haben sich Data-Lake-Systeme hervorgetan.
Ein Data Lake ist eine Methodik, die die Erfassung, Verfeinerung, Archivierung und Erkundung von Daten verbessert \parencite{datalake_01}.
Große Datenmengen sollen möglichst kostensparend gespeichert werden.
Verschiedenste Formate können in einem Data Lake abgelegt und verarbeitet werden.
Darunter zählen zum Beispiel strukturierte Daten aus traditionellen Datenbanksystemen, semistrukturierte Daten wie JSON-Dateien, bei denen nicht alle Attribute festgelegt sind und unstrukturierte Daten wie Bilder oder Videos \parencite{datalake_02}.


\section{Zielsetzung der Arbeit}

In einer Reihe von Projekten und Masterarbeiten wird an der Hochschule Niederrhein ein generelles Data-Lake-System entwickelt.
Das heißt es ist überall einsetzbar und alle nötigen Komponenten sind im System enthalten.
Ein Prototyp wurde bereits während eines Masterprojekts entwickelt \parencite{prototyp}.

Diese Masterarbeit befasst sich mit der Entwicklung, Implementierung und Integration einer neuen Schnittstelle für die Aufnahme von Daten (Ingestion).
Die wichtigsten Ziele sind dabei die Generalität, Kontinuität und Datenversionierung bei einer möglichst einfachen Verwendung für den Benutzer.
Die Generalität bezeichnet die Funktionalität unabhängig von der Art der Datenquelle oder der Struktur der Daten.
Unter Kontinuität ist das erneute und optional auch regelmäßige Nachladen von Daten aus einer Quelle gemeint.
Für diese Daten soll es auch möglich sein eine Versionierung durch zu führen um Änderungen auswerten zu können.

Der Aufbau der Arbeit gliedert sich in fünf Kapitel.
In diesem Kapitel werden wichtige Grundlagen für die Arbeit vermittelt und verwandte Arbeiten erläutert.
Danach wird auf die Ziele und die daraus folgenden Anforderungen an die Schnittstelle eingegangen.
Das nächste Kapitel befasst sich mit der Entwicklung einer Architektur und des Designs der Ingestion.
Auf diesem Design baut die darauf folgende Umsetzung auf.
Zuletzt wird das System durch funktionale Tests und Benchmarks evaluiert.

\section{Grundlagen und Verwandte Arbeiten}

% Techniken
\input{Kapitel/Einleitung/Spark}
\input{Kapitel/Einleitung/HDFS}
\input{Kapitel/Einleitung/Kafka}
\input{Kapitel/Einleitung/DeltaLake}

% Arbeiten
\input{Kapitel/Einleitung/Prototyp}
\input{Kapitel/Einleitung/CDC}

% \input{Kapitel/Einleitung/Datalake}

